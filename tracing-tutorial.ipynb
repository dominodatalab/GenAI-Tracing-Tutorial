{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TriageFlow: Incident Triage Demo\n",
    "\n",
    "Multi-agent incident triage with Domino GenAI tracing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Save your API key as a Domino user environment variable:\n",
    "1. **Account Settings** → **User Environment Variables**\n",
    "2. Add `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Provider & Vertical\n",
    "\n",
    "Choose your LLM provider and industry vertical for sample incidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_dropdown = widgets.Dropdown(\n",
    "    options=[\"openai\", \"anthropic\"],\n",
    "    value=\"openai\",\n",
    "    description=\"Provider:\"\n",
    ")\n",
    "\n",
    "vertical_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        (\"Financial Services\", \"financial_services\"),\n",
    "        (\"Healthcare\", \"healthcare\"),\n",
    "        (\"Energy\", \"energy\"),\n",
    "        (\"Public Sector\", \"public_sector\")\n",
    "    ],\n",
    "    value=\"financial_services\",\n",
    "    description=\"Vertical:\"\n",
    ")\n",
    "\n",
    "display(provider_dropdown, vertical_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration\n",
    "\n",
    "All prompts, model settings, and agent parameters are centralized in `config.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "provider = provider_dropdown.value\n",
    "model = config[\"models\"][provider]\n",
    "print(f\"Provider: {provider}\\nModel: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Client & Auto-Tracing\n",
    "\n",
    "MLflow's `autolog()` automatically captures all LLM calls without additional instrumentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Disable inline trace display in notebook\n",
    "mlflow.tracing.disable_notebook_display()\n",
    "\n",
    "if provider == \"openai\":\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    mlflow.openai.autolog()\n",
    "else:\n",
    "    from anthropic import Anthropic\n",
    "    client = Anthropic()\n",
    "    mlflow.anthropic.autolog()\n",
    "\n",
    "print(f\"Auto-tracing enabled for {provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Domino Tracing\n",
    "\n",
    "- `add_tracing`: Decorator for capturing inputs, outputs, and evaluation metrics\n",
    "- `DominoRun`: Context manager for aggregating metrics across multiple traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from domino.agents.tracing import add_tracing\n",
    "from domino.agents.logging import DominoRun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models, Agents, and Judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import Incident, IncidentSource\n",
    "from src.agents import classify_incident, assess_impact, match_resources, draft_response\n",
    "from src.judges import judge_classification, judge_response, judge_triage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_evaluator(span) -> dict:\n",
    "    \"\"\"Extract pre-computed metrics from pipeline outputs.\"\"\"\n",
    "    outputs = span.outputs or {}\n",
    "    if not hasattr(outputs, \"get\"):\n",
    "        return {}\n",
    "\n",
    "    # Scores are pre-computed inside triage_incident\n",
    "    return {\n",
    "        \"classification_confidence\": outputs.get(\"classification_confidence\", 0.5),\n",
    "        \"impact_score\": outputs.get(\"impact_score\", 5.0),\n",
    "        \"resource_match_score\": outputs.get(\"resource_match_score\", 0.5),\n",
    "        \"completeness_score\": outputs.get(\"completeness_score\", 0.5),\n",
    "        \"classification_judge_score\": outputs.get(\"classification_judge_score\", 3),\n",
    "        \"response_judge_score\": outputs.get(\"response_judge_score\", 3),\n",
    "        \"triage_judge_score\": outputs.get(\"triage_judge_score\", 3),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Traced Pipeline\n",
    "\n",
    "The `@add_tracing` decorator creates a single trace tree per incident. Each agent runs as a nested span with:\n",
    "- Function inputs and outputs\n",
    "- LLM calls captured via autolog (showing span types like `ChatCompletion`)\n",
    "- Evaluation metrics attached to the trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_tracing(name=\"triage_incident\", autolog_frameworks=[provider], evaluator=pipeline_evaluator)\n",
    "def triage_incident(incident: Incident):\n",
    "    \"\"\"Run the 4-agent triage pipeline with LLM judges.\"\"\"\n",
    "    # Run agents\n",
    "    classification = classify_incident(client, provider, model, incident, config)\n",
    "    impact = assess_impact(client, provider, model, incident, classification, config)\n",
    "    resources = match_resources(client, provider, model, classification, impact, config)\n",
    "    response = draft_response(client, provider, model, incident, classification, impact, resources, config)\n",
    "\n",
    "    # Convert to dicts for judges\n",
    "    class_dict = classification.model_dump()\n",
    "    impact_dict = impact.model_dump()\n",
    "    resources_dict = resources.model_dump()\n",
    "    response_dict = response.model_dump()\n",
    "    primary = resources_dict.get(\"primary_responder\", {})\n",
    "\n",
    "    # Run judges inside trace context\n",
    "    class_judge = judge_classification(client, provider, incident.description, class_dict)\n",
    "    \n",
    "    comms = response_dict.get(\"communications\", [])\n",
    "    if comms:\n",
    "        resp_judge = judge_response(client, provider, incident.description, class_dict.get(\"urgency\", 3), comms[0])\n",
    "    else:\n",
    "        resp_judge = {\"score\": 1}\n",
    "    \n",
    "    triage_judge = judge_triage(client, provider, incident.description, class_dict, impact_dict, resources_dict, response_dict)\n",
    "\n",
    "    return {\n",
    "        \"classification\": classification,\n",
    "        \"impact\": impact,\n",
    "        \"resources\": resources,\n",
    "        \"response\": response,\n",
    "        # Metrics for evaluator\n",
    "        \"classification_confidence\": class_dict.get(\"confidence\", 0.5),\n",
    "        \"impact_score\": impact_dict.get(\"impact_score\", 5.0),\n",
    "        \"resource_match_score\": primary.get(\"match_score\", 0.5) if isinstance(primary, dict) else 0.5,\n",
    "        \"completeness_score\": response_dict.get(\"completeness_score\", 0.5),\n",
    "        \"classification_judge_score\": class_judge.get(\"score\", 3),\n",
    "        \"response_judge_score\": resp_judge.get(\"score\", 3),\n",
    "        \"triage_judge_score\": triage_judge.get(\"score\", 3),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Incidents\n",
    "\n",
    "Example incidents will be loaded from the vertical selected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical = vertical_dropdown.value\n",
    "df = pd.read_csv(f\"example-data/{vertical}.csv\")\n",
    "print(f\"Loaded {len(df)} incidents from {vertical}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_incident(row) -> Incident:\n",
    "    return Incident(\n",
    "        ticket_id=row[\"ticket_id\"],\n",
    "        description=row[\"description\"],\n",
    "        source=IncidentSource(row[\"source\"]),\n",
    "        reporter=row[\"reporter\"] if pd.notna(row[\"reporter\"]) else None,\n",
    "        affected_system=row[\"affected_system\"] if pd.notna(row[\"affected_system\"]) else None,\n",
    "        initial_severity=int(row[\"initial_severity\"]) if pd.notna(row[\"initial_severity\"]) else None\n",
    "    )\n",
    "\n",
    "incidents = [row_to_incident(row) for _, row in df.iterrows()]\n",
    "print(f\"Loaded {len(incidents)} incidents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Triage Pipeline\n",
    "\n",
    "`DominoRun` aggregates metrics across all traces in the batch via `custom_summary_metrics`. Supported aggregations: `mean`, `median`, `stdev`, `min`, `max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment and run naming\n",
    "username = os.environ.get(\"DOMINO_USER_NAME\", os.environ.get(\"USER\", \"demo_user\"))\n",
    "project_name = os.environ.get(\"DOMINO_PROJECT_NAME\", \"default\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "experiment_name = f\"tracing-{project_name}-{username}\"\n",
    "run_name = f\"{vertical}-{username}-{timestamp}\"\n",
    "\n",
    "aggregated_metrics = [\n",
    "    # Base metrics\n",
    "    (\"classification_confidence\", \"mean\"),\n",
    "    (\"impact_score\", \"median\"),\n",
    "    (\"resource_match_score\", \"mean\"),\n",
    "    (\"completeness_score\", \"mean\"),\n",
    "    # Judge scores\n",
    "    (\"classification_judge_score\", \"mean\"),\n",
    "    (\"response_judge_score\", \"mean\"),\n",
    "    (\"triage_judge_score\", \"mean\"),\n",
    "]\n",
    "\n",
    "print(f\"Experiment: {experiment_name}\")\n",
    "print(f\"Run: {run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "results = []\n",
    "run_id = None\n",
    "\n",
    "with DominoRun(agent_config_path=\"config.yaml\", custom_summary_metrics=aggregated_metrics) as run:\n",
    "    # Set run name via MLflow\n",
    "    mlflow.set_tag(\"mlflow.runName\", run_name)\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    for incident in incidents:\n",
    "        print(f\"Processing {incident.ticket_id}...\")\n",
    "        \n",
    "        result = triage_incident(incident)\n",
    "        \n",
    "        results.append({\n",
    "            \"ticket_id\": incident.ticket_id,\n",
    "            **result\n",
    "        })\n",
    "        print(f\"  → {result['classification'].category.value} | Urgency: {result['classification'].urgency} | Impact: {result['impact'].impact_score}\")\n",
    "    \n",
    "    # Suppress DominoRun exit messages\n",
    "    _stdout = sys.stdout\n",
    "    sys.stdout = io.StringIO()\n",
    "\n",
    "sys.stdout = _stdout\n",
    "print(f\"\\nProcessed {len(results)} incidents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame([{\n",
    "    \"Ticket\": r[\"ticket_id\"],\n",
    "    \"Category\": r[\"classification\"].category.value,\n",
    "    \"Urgency\": r[\"classification\"].urgency,\n",
    "    \"Impact\": r[\"impact\"].impact_score,\n",
    "    \"Responder\": r[\"resources\"].primary_responder.name,\n",
    "    \"SLA Met\": r[\"resources\"].sla_met\n",
    "} for r in results])\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Communication\n",
    "\n",
    "Each incident generates tailored communications for technical teams, management, and affected users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = results[0]\n",
    "print(f\"Ticket: {sample['ticket_id']}\\n\")\n",
    "for comm in sample[\"response\"].communications:\n",
    "    print(f\"--- {comm.audience.upper()} ---\")\n",
    "    print(f\"Subject: {comm.subject}\")\n",
    "    print(f\"{comm.body[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ad Hoc Evaluations\n",
    "\n",
    "Add evaluations after traces are generated using `search_traces()` to retrieve traces from the run and `log_evaluation()` to attach scores to specific traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from domino.aisystems.tracing import search_traces, log_evaluation\n",
    "from domino.agents.tracing import search_traces\n",
    "from domino.agents.logging import log_evaluation\n",
    "\n",
    "\n",
    "# Retrieve all traces from the run\n",
    "traces = search_traces(run_id=run_id)\n",
    "\n",
    "# Add custom evaluations to each trace based on triage results\n",
    "for i, trace in enumerate(traces.data):\n",
    "    result = results[i]\n",
    "    \n",
    "    # Compute combined quality score from judge evaluations\n",
    "    combined_quality = (\n",
    "        result[\"classification_judge_score\"] +\n",
    "        result[\"response_judge_score\"] +\n",
    "        result[\"triage_judge_score\"]\n",
    "    ) / 3\n",
    "    \n",
    "    # Flag high-urgency incidents that may need manual review\n",
    "    needs_review = result[\"classification\"].urgency >= 4 and result[\"impact\"].impact_score >= 7\n",
    "    \n",
    "    log_evaluation(trace_id=trace.id, name=\"combined_quality_score\", value=round(combined_quality, 2))\n",
    "    log_evaluation(trace_id=trace.id, name=\"needs_manual_review\", value=1.0 if needs_review else 0.0)\n",
    "\n",
    "print(f\"Added evaluations to {len(traces.data)} traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Open **Domino Experiment Manager** to view:\n",
    "- Execution flow across all 4 agents\n",
    "- Inline evaluation metrics per trace\n",
    "- Aggregated statistics across the batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
