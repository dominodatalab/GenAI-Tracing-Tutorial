name: triageflow-judges
version: "1.0"
description: Combined LLM-as-Judge configurations for evaluating TriageFlow agents

# =============================================================================
# GLOBAL JUDGE SETTINGS
# =============================================================================
defaults:
  model: "gpt-4o-mini"
  temperature: 0.1
  max_tokens: 500
  response_format: "json"

# =============================================================================
# PER-AGENT EVALUATION CRITERIA
# =============================================================================
evaluators:
  classifier:
    criteria:
      - name: json_validity
        type: binary
        weight: 0.2
        prompt: |
          Is this a valid JSON response with all required fields?
          Required: category, subcategory, urgency, confidence, reasoning, affected_domain
          Response: {agent_output}
          Answer only: {"valid": true} or {"valid": false, "missing": ["field1", "field2"]}

      - name: category_accuracy
        type: binary
        weight: 0.3
        prompt: |
          Ground truth category: {expected_category}
          Agent predicted: {predicted_category}
          Is the prediction correct? {"correct": true/false}

      - name: urgency_accuracy
        type: scored
        scale: [0, 1, 2]
        weight: 0.3
        prompt: |
          Ground truth urgency: {expected_urgency}
          Agent predicted: {predicted_urgency}
          Score: 2 if exact match, 1 if off by 1 level, 0 if off by 2+
          Return: {"score": N, "reason": "brief explanation"}

      - name: reasoning_quality
        type: scored
        scale: [1, 2, 3]
        weight: 0.2
        prompt: |
          Evaluate the reasoning provided for this classification:

          Incident: {incident}
          Classification: {classification}
          Reasoning: {reasoning}

          Score 1-3:
          1 = Poor: Vague, doesn't reference incident details
          2 = Acceptable: References some details, logical
          3 = Good: Specific references, clear logic chain

          Return: {"score": N, "reason": "brief explanation"}

  impact_assessor:
    criteria:
      - name: json_validity
        type: binary
        weight: 0.15
        prompt: |
          Is this valid JSON with required fields?
          Required: impact_score, affected_users_estimate, affected_systems, blast_radius, reasoning
          Response: {agent_output}
          Answer: {"valid": true/false}

      - name: impact_score_range
        type: binary
        weight: 0.25
        prompt: |
          Expected impact range: {expected_impact_min} to {expected_impact_max}
          Agent score: {predicted_impact}
          Is the score within expected range? {"in_range": true/false}

      - name: tool_usage
        type: binary
        weight: 0.25
        prompt: |
          Did the agent properly use both required tools?
          Required: lookup_historical_incidents, calculate_impact_score
          Tools called: {tools_called}
          Answer: {"correct_usage": true/false, "missing": []}

      - name: blast_radius_reasonableness
        type: scored
        scale: [1, 2, 3]
        weight: 0.35
        prompt: |
          Given this incident and impact assessment, is the blast_radius reasonable?

          Incident: {incident}
          Affected users: {affected_users}
          Affected systems: {affected_systems}
          Blast radius: {blast_radius}

          1 = Unreasonable (clearly over/under-estimated)
          2 = Acceptable (reasonable interpretation)
          3 = Accurate (well-justified given the data)

          Return: {"score": N, "reason": "brief"}

  resource_matcher:
    criteria:
      - name: json_validity
        type: binary
        weight: 0.2
        prompt: |
          Valid JSON with: primary_responder, backup_responders, sla_target_hours, sla_met, escalation_path?
          Response: {agent_output}
          Answer: {"valid": true/false}

      - name: skill_match
        type: scored
        scale: [1, 2, 3]
        weight: 0.4
        prompt: |
          Incident category: {category}
          Required skills (inferred): {inferred_skills}
          Primary responder skills: {responder_skills}

          1 = Poor match (missing critical skills)
          2 = Partial match (has some relevant skills)
          3 = Good match (has key required skills)

          Return: {"score": N, "reason": "brief"}

      - name: sla_consistency
        type: binary
        weight: 0.4
        prompt: |
          Urgency: {urgency}
          Defined SLA: {defined_sla_hours}
          Agent SLA target: {agent_sla_hours}

          Is the agent's SLA target consistent with the defined SLAs?
          Return: {"consistent": true/false}

  response_drafter:
    criteria:
      - name: json_validity
        type: binary
        weight: 0.15
        prompt: |
          Valid JSON with: communications, action_items, estimated_resolution_time, escalation_required?
          Response: {agent_output}
          Answer: {"valid": true/false}

      - name: audience_coverage
        type: scored
        scale: [1, 2, 3]
        weight: 0.3
        prompt: |
          Impact score: {impact_score}
          Blast radius: {blast_radius}
          Communications sent to: {audiences}

          1 = Missing critical stakeholders
          2 = Covers main stakeholders
          3 = Comprehensive coverage appropriate to impact

          Return: {"score": N, "reason": "brief"}

      - name: tone_appropriateness
        type: scored
        scale: [1, 2, 3]
        weight: 0.25
        prompt: |
          Urgency level: {urgency}
          Communication samples:
          {communication_samples}

          1 = Tone mismatched to urgency (too casual for critical, too alarming for minor)
          2 = Acceptable tone
          3 = Tone well-calibrated to urgency

          Return: {"score": N, "reason": "brief"}

      - name: actionability
        type: scored
        scale: [1, 2, 3]
        weight: 0.3
        prompt: |
          Action items: {action_items}

          1 = Vague or missing actions
          2 = Some actionable items
          3 = Clear, specific, assignable actions

          Return: {"score": N, "reason": "brief"}

# =============================================================================
# HOLISTIC JUDGES (run after full pipeline)
# =============================================================================
holistic_judges:
  classification_judge:
    description: Evaluates the quality of incident classification
    temperature: 0.1
    max_tokens: 300
    prompt: |
      Evaluate this incident classification. Score 1-5 (5=excellent).

      Incident: {incident}

      Classification:
      - Category: {category}
      - Subcategory: {subcategory}
      - Urgency: {urgency}
      - Confidence: {confidence}
      - Reasoning: {reasoning}

      Evaluate:
      1. Is the category appropriate for this incident?
      2. Is the urgency level justified by the description?
      3. Is the reasoning sound and specific?
      4. Is the confidence level calibrated appropriately?

      Return JSON:
      {
          "score": <1-5>,
          "category_appropriate": <true/false>,
          "urgency_justified": <true/false>,
          "reasoning_quality": <1-3>,
          "rationale": "<brief explanation>"
      }

  response_judge:
    description: Evaluates the quality of incident response communications
    temperature: 0.1
    max_tokens: 300
    prompt: |
      Evaluate this incident communication. Score 1-5 (5=excellent).

      Incident: {incident}
      Urgency Level: {urgency}

      Communication to {audience}:
      Subject: {subject}
      Body: {body}

      Evaluate:
      1. Is the tone appropriate for the audience and urgency?
      2. Is the information clear and complete?
      3. Does it convey appropriate urgency?
      4. Are the next steps actionable?

      Return JSON:
      {
          "score": <1-5>,
          "tone_appropriate": <true/false>,
          "information_clear": <true/false>,
          "urgency_conveyed": <true/false>,
          "actionable": <true/false>,
          "rationale": "<brief explanation>"
      }

  triage_judge:
    description: Evaluates the overall quality of a complete triage decision
    temperature: 0.1
    max_tokens: 400
    prompt: |
      Evaluate this complete incident triage. Score 1-5 (5=excellent).

      Incident: {incident}

      Triage Summary:
      - Category: {category} (Urgency: {urgency})
      - Impact Score: {impact_score}/10, Blast Radius: {blast_radius}
      - Primary Responder: {responder} (Match Score: {match_score})
      - SLA Met: {sla_met}
      - Action Items: {action_count}
      - Escalation Required: {escalation}

      Evaluate the overall triage quality:
      1. Is the classification appropriate for the incident?
      2. Is the impact assessment reasonable?
      3. Is the resource assignment logical given the incident type?
      4. Is the response plan comprehensive?

      Return JSON:
      {
          "score": <1-5>,
          "classification_quality": <1-3>,
          "impact_assessment_quality": <1-3>,
          "resource_assignment_quality": <1-3>,
          "response_plan_quality": <1-3>,
          "rationale": "<brief explanation>"
      }

# =============================================================================
# AD HOC EVALUATION THRESHOLDS
# =============================================================================
thresholds:
  urgency_threshold: 4
  impact_threshold: 7
  quality_threshold: 3.5
  needs_manual_review_when:
    - "urgency >= urgency_threshold AND impact_score >= impact_threshold"
    - "combined_quality_score < quality_threshold"
